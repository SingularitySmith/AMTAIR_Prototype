\documentclass[12pt, a4paper]{article}

\title{Trusting Artificial Intelligence}      %   The Title of your seminarpaper
\author{Valentin Jakob Meyer}               %   Your Name
\date{June 2021}

%packages used
    \usepackage[utf8]{inputenc}
    
    \usepackage{amssymb,amsmath,amsfonts}   % Fr mathematische Darstellungen
    
    \usepackage{graphicx}
    
    \usepackage{threeparttable}
    
    \usepackage{float}
    
    \usepackage{setspace}\onehalfspacing    % Zeilenabstand
    
    \usepackage{comment}
    
    \usepackage{pdfpages}
    
	\usepackage{enumitem}

    \usepackage[margin=1in,includefoot]{geometry}
    
    \usepackage[autostyle]{csquotes}
    
\usepackage[sfdefault]{roboto}
\usepackage[T1]{fontenc}

    \usepackage{setspace}
    \setstretch{1.25}

    \usepackage{fancyhdr}
    % Turn on the style
    \pagestyle{fancy}
    % Clear the header and footer
    \fancyhead{}
    \fancyfoot{}
    % Set the right side of the footer to be the page number
    \fancyfoot[R]{\thepage}

\usepackage{biblatex-chicago}
\usepackage{csquotes}

%	\usepackage[backend=biber, style=authoryear, natbib=true, url=false, doi=true, eprint=false ]{biblatex} 		%for Overleaf
%	\usepackage[backend=bibtex, style=authoryear, natbib=true, url=false, doi=true, eprint=false ]{biblatex} 		%for TeXstudio

	\addbibresource{PhilAI.bib}
	
	    \newread\tmp
	    
	    \newcommand{\quickcharcount}[1]{%
	      \immediate\write18{texcount -1 -sum -merge -char #1.tex > #1-chars.sum}%
	      \openin\tmp=#1-chars.sum%
	      \read\tmp to \thechar%
	      \closein\tmp%
	    }
	    
	    \newcommand{\quickwordcount}[1]{%
	      \immediate\write18{texcount -1 -sum -merge #1.tex > #1-words.sum}%
	      \openin\tmp=#1-words.sum%
	      \read\tmp to \theword%
	      \closein\tmp%
	    }
	    
	    \newcommand{\charactercount}[1]{
	    \immediate\write18{
	        expr `texcount -1 -sum -merge #1.tex` + `texcount -1 -sum -merge -char #1.tex` - 1 
	        > chars.txt
	    }\input{chars.txt}}
    
    \usepackage[]{hyperref} \hypersetup{ colorlinks=true, }
    \hypersetup{
        colorlinks=true,
        linkcolor=LMU,
        filecolor=LMU,      
        urlcolor=LMU,
        citecolor=LMU
    }
    
    \usepackage{xcolor}
        \newcommand{\black}{\color{black}}
        \newcommand{\LMU}{\color{LMU}}
        \newcommand{\red}{\color{red}}
    \definecolor{LMU}{RGB}{0,148,64}
    
    
    \urlstyle{same}

\begin{comment}
    
  
    %----------------------------------------------------------------------------------------
    %	Glossary
    %----------------------------------------------------------------------------------------
\usepackage[toc]{glossaries}            %   Must be after the hyperref package
\makeglossaries
\newglossaryentry{latex}
{
    name=latex,
    description={
    \ref{latex for students}
    $\theta$    \\
(Is a mark up language specially suited for scientific documents)
}
}

\newglossaryentry{maths}                    %   This will create a new glossary entry, use the term you will use often
{
    name=maths,
    description={
    \ref{mathematics for economists}            %   This will create the link to the (sub)section where \label{"whatever you reference"} was created
    $\Gamma$ \\                                   %   This can be used to include a 
    (Mathematics is what mathematicians do)     %   Optional explanation that will appear in the Appendix
}
}
\end{comment}  
    %----------------------------------------------------------------------------------------
    %----------------------------------------------------------------------------------------

    %----------------------------------------------------------------------------------------
    %	Table of Content settings
    %----------------------------------------------------------------------------------------
    \setcounter{tocdepth}{5}            %       Up to which level headings are displayed in the TOC
    \setcounter{secnumdepth}{1}         %       Up to which level headings are numbered in the document
    %----------------------------------------------------------------------------------------
    %----------------------------------------------------------------------------------------


    %----------------------------------------------------------------------------------------
    %	Background picture/ Watermark
    %----------------------------------------------------------------------------------------
    \usepackage{eso-pic}
    \newcommand\BackgroundPic{%
    \put(0,0){%
    \parbox[b][\paperheight]{\paperwidth}{%
    \vfill
%    \centering
    \includegraphics[width=45mm,%
    keepaspectratio]{backgroundsealwatermark.png}%
    \vfill
    }}}
    %----------------------------------------------------------------------------------------
    %----------------------------------------------------------------------------------------
  
\begin{document}
%    \AddToShipoutPicture*{\BackgroundPic}

    \begin{comment}     %       general consiterations for writing a seminar thesis
    General purpose/considerations/goals\\
    	Summarize the paper and conduct a critical review 
     %   		(...kritisch Würdigen)
        Boil down complex arguments/lines of thought, give the intuitions and explain the context 
    %        	(...in Zusammenhang einordnen)
        Carve out the methodological approach and clearly describe the practical implications
    %        	(Methodische Fragestellung klar herausarbeiten und den Anwendungsbezug deutlich beschreiben)
                
        All statements/claims (relevant for your paper) need to be justified 
        	They have to be supported by arguments 
           	Just listing sources that support the claim is insufficient
           	Introduce equations/assumptions in an order that allows them to build on oneanother
           	    Basic to specific
           	
        Write, as if your audience were fellow Economics students
\end{comment}


    \begin{titlepage}
    \newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
    \flushright  \includegraphics[width = 40mm]{backgroundsealwatermark.png}\\[2cm]

    \center % Center everything on the page
    %----------------------------------------------------------------------------------------
    %	HEADING SECTIONS
    %----------------------------------------------------------------------------------------
    \textsc{\LARGE University of Bayreuth}\\[0.75cm] % Name of your university/college
%    \textsc{\Large Insurance Economics}\\[0.5cm] % Major heading such as course name
    \textsc{\large  Introduction to the \\ Philosophy of Artificial Intelligence}\\[1.5cm]
%    \textsc{\large by Shlomo Benartzi and Richard H. Thaler}\\[0.5cm] 
    % Minor heading such as course title
    %----------------------------------------------------------------------------------------
    %	TITLE SECTION
    %----------------------------------------------------------------------------------------
    \HRule \\[0.4cm]
    { \huge \bfseries  Challenges in the Public Debate on Artificial General Intelligence\par } % Title of your document, suggestion for the Title of your Thesis and Author
    \vspace{0.8cm}
    \HRule \\[1.5cm]
    %----------------------------------------------------------------------------------------
    %	AUTHOR SECTION
    %----------------------------------------------------------------------------------------
    \begin{minipage}{0.4\textwidth}
    \begin{flushleft} \large
    \emph{Author:}\\ 
        Valentin Jakob \textsc{Meyer}	\\         %   Your name
%        Biedersteinerstraße 30 \\               %   Street address
%       80805 München\\                         %   Postal code and city
        Philosophy \& Economics M.A.\\                       %   Program (subject of studies)
        Second Semester \\                      %   Your current Semester
        Student-Id: \textsc{Bt720462}                  %   Martrikelnummer
    \end{flushleft}
    \end{minipage}
    ~
    \begin{minipage}{0.4\textwidth}
    \begin{flushright} \large
       \emph{Word count:} \\
        4.000 \\                                %	Include character count (below 30.000)
    \vspace{\baselineskip}
    \emph{Supervisor:} \\ 
        Ph.D.\\
        Carlos \textsc{Núñez} %   \&  \\
    
    \end{flushright}
    \end{minipage}\\[2cm]
    % If you don't want a supervisor, uncomment the two lines below and remove the section above
    %\Large \emph{Author:} 
    %John \textsc{Smith} [3cm]               % Your name
    %----------------------------------------------------------------------------------------
    %	DATE SECTION
    %----------------------------------------------------------------------------------------
    {\large September, 2022}\\[2cm]             % Date, change the \today to a set date if you want to be precise 
    
    %----------------------------------------------------------------------------------------
    %	LOGO SECTION
    %----------------------------------------------------------------------------------------
    %\includegraphics[width=0.2\textwidth]{Siegel_35s.png} % Include a department/university logo - this will require the graphicx package
    
    %----------------------------------------------------------------------------------------
    
    \vfill                                  % Fill the rest of the page with whitespace
    \end{titlepage}


%	\begin{abstract}  %allowed/needed? 

%	\end{abstract}

\newpage  \tableofcontents      %   has to include page numbers
\newpage

%\newpage \part{Introduction}

    \begin{quote}
            \textit{“Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.” }
        \begin{flushright}
            \textsc{--Irving John Good, in 1963}   \footcite{good1966speculations}   \textit{Speculations Concerning the First Ultraintelligent Machine} 
        \end{flushright}
    \end{quote}

		\subsubsection{Introduction}
			Despite the stark differences between the technological and social realities of the past and present, historically many scientists , artists, and futurists were able to foresee future technologies.
			Our 'hindsight-bias' favors identifying those predictions which came to fruition, but sometimes the anticipation of developments that manifest, is grounded in the robustness and soundness of the reasoning deployed.
			Goods' quote from the 1960's captured the essence, of today's quasi-consensus\footcite{russell2009ethics} among artificial intelligence safety and alignment researchers about the mechanism by which humanity threatens itself through the advancement of artificial intelligence (AI).
			
			This essay aims to retrace the case for existential risk from 'artificial general intelligence' (AGI) and to highlight epistemic challenges in the debates about deployment of such potentially transformative AI systems.
			The central thesis is that expected (western, liberal) political debates about the align- and deployment of potentially transformative AGI, are inadequate as collective decision procedures because their epistemic flaws drastically reduce any chances at solving the ‘\hyperlink{alignmentp}{alignment problem}'.
			
			This paper begins by retracing the case for why and how the deployment of a general artificial intelligence will be transformative.
			To do so we first define the key terms and concepts, specifically 'transformative artificial general intelligence', 'superintelligence', the orthogonality thesis, the instrumental convergence thesis, the alignment problem and the 'unilateralist's curse'.
			
			Next, we consider examples for epistemic problems of the expected political debate:
			\begin{enumerate}
				\item the slowness of political decision processes
				\item the direction of burden of proof required for the prohibition of technologies
				\item our conception of separate jurisdictions
				\item the strained relationship between public and expert opinions
				\item the political influence granted to economic interests
			\end{enumerate}
			The explanations of these mechanisms support the claim that \textit{our current collective procedures are inadequate for addressing challenges of the nature of transformative artificial intelligence}, which holds especially true for western, liberal democracies.
			
			The academic literature has revealed a variety of ways in which AGI could cause existential threats.
			Examples include\textit{ weaponization of AI},\textit{ AGI that is designed to be malevolent}, \textit{preemptive nuclear strikes aiming to prevent the development of AGI} and \textit{AI arms races}.
			This essay addresses only the "pure" case of risk from the nature of 'superintelligence' and AGI, and leaves aside all such mechanisms.
			Focussing on this simplified and restricted case helps to establish a lower and preliminary bound for the difficulties to be expected in the debates about dangers from AGI.
			
			
		\subsubsection{The nature of AGI}
			We begin by clarifying some definitions and terminology to better understand the nature of AGI, contextualize it and grasp its implications.
			
			Over the past decade, the literature\footcite{armstrong2013general} on AGI made crucial progress by separating the debate around AGI and superintelligence from the more traditional debates about 'intentionality' and 'first-person consciousness'.
			Philosophical concerns such as the 'Chinese room experiment' and the 'hard problem of consciousness' appear tangential to today's empirical AI research, which attempts to create algorithms that optimize for arbitrary goals\footcite{bostrom2012superintelligent}.
			Understanding AGI and superintelligence in terms 'general dominance at goal-oriented behavior'\footnote{\cite{bostrom2014superintelligence} \label{bostrom}} differentiates against normatively stronger conceptions of intelligence  that include moral wisdom or adherence to some standards of moral reasoning.
			
			\paragraph[Definition of artificial intelligence]{Artificial intelligence (AI)}
				is then --- without reference to morality --- understood as "\textit{optimization processes, that strictly take whatever actions are 	judged most likely to accomplish its (possibly complicated and implicit) goals}"\textsuperscript{\ref{bostrom}}.

			\paragraph[Definition of transformative AGI]{Artificial general intelligence (AGI)} 
				is defined as\footcite{goertzel2007artificial} the level and type of intelligence required for agents to surpass humans at understanding and learning.
				This capacity is commonly contrasted with "narrow AI" which may surpass human ability in certain tasks but lacks the domain generality of general cognitive abilities.
			
			\paragraph[Definition of superintelligence]{'Superintelligence'}
				is defined by \cite{bostrom2014superintelligence} as \textit{"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest"}.

			\paragraph{The source of power of AGI}
				in machines is the lack of biological constraints and limits on their cognitive abilities.
				Some of the benefits that digital computation has, are the significantly faster information processing, an orders of magnitude larger knowledge base (internet), multitasking (multi-threading) and the ability for perfect recall\footcite{bostrom2014superintelligence}.
				From the previous definitions it also follows that 'superintelligences' are a subset of AGIs, characterized by the surpassing of human minds in \textit{all} abilities.
				This distinction is relevant only in that AGI is defined more technically while superintelligence can be related to more intuitively.
				The two concepts will therefore be considered interchangeable.
				
				Next, we explain the core concepts and elaborate where the dangers from AGI come from.

			\paragraph[Orthogonality Thesis]{The Orthogonality Thesis}
				as stated in \cite{bostrom2012superintelligent} argues that \textit{any level of intelligence could be combined with more or less}\footnote{Aside from some technical caveats, namely constraints from motivation and dynamical constraints. For example, that more complex goals require a sufficient degree of intelligence to comprehend them and that intelligent minds with desires to be stupid might not remain smart for long.} \textit{any final goal}.
				
				In this conception, intelligence and final goals are orthogonal dimensions along which the minds, that characterize possible agents, may vary freely.
				
				This contradicts the common belief that superintelligences created by humans will "discover" moral truths which are compatible with human values.			
				\textcite{armstrong2013general} argues, that even if moral facts, which can be proven by any rational agent, exist, agents could still be created with arbitrary final goals because the could have instrumental reasons to avoid discovering any truths that hinder them from obtaining their original goals.
			
			\paragraph[Convergence Thesis]{The Convergence Thesis}
				in its popular form originates from Omohundro's\footcite{Omohundro_thebasic} work on 'basic AI drives'.
				It postulates the tendency of intelligent agents to pursue a set of roughly similar sub goals even if their ultimate goals are fundamentally different.
				That some goals are preconditions for other goals, gives rise to this instrumental convergence.
				There might exist sub goals that AGIs would pursue to which we are oblivious due to our cognitive limitations.
				Nevertheless, the literature\footnote{Refer for example to \textcite{russell2009ethics} for an overview.} has identified a number of strong candidates:
				\begin{enumerate}
				\item freedom from interference
				\item self-protection and preservation
				\item self improvement
				\item maximization of implicit utility functions
				\item goal-content integrity
				\item insatiably acquisition of additional resources
				\end{enumerate}
				It is intuitive to see that such goals are akin to necessary conditions in the pursuit of some particular end, without themselves being end goals.
				
				It is also trivial to see how such goals conflict with what humans value.
				Imagine a superintelligence with an apparently harmless goal, such as calculating more and more digits of Pi\footnote{This is a standard example \textcite{bostrom2014superintelligence} uses to visualize the threat of an AGIs insatiable resources acquisition.}.
				Such a harmless goal would by default motivate an AGI to act in a severely harmful manner, by causing it to see humanity as a threat to both, its instrumental and by extension to its ultimate goal. 
			
			\paragraph{The alignment problem}\label{alignmentp}
				captures the \textit{fundamental and existential} problem inherent to AGI.
				Contrary to other technologies, which are usually dangerous when in the wrong hands, AGI is dangerous by itself or "in its own hands".
				
				
				Alignment is the degree to which AIs work towards their designers' \textit{intended} goals and interests.
				It can be thought of as the 'alignment' between what an AI does and how it goes about doing so and what the designer actually values.
				
				Problems that have already been identified\footcite{russell2009ethics} and that are expected to exponentially worsen\footcite{carlsmith22} as the capacities of AIs increase are:
				\begin{enumerate}
					\item proxy goals that omit desired constraints
					\item emergent goals that only become apparent when systems are confronted with new data or situations
					\item reward hacking
					\item unintended side-effects
					\item difficulty to completely specify all (un-)desired behavior
					\item power-seeking behavior
				\end{enumerate}
				Another mechanism, referred to as "treacherous turn", which aggravates the alignment problem, is the reaction of an superintelligence to any scenario where it might expect to be labeled as "malfunctioning".
				In such cases it would anticipate human interference or the attempt to shut it off and deploy its superior intellect to outmaneuver any such attempts.
				
				While all these critical problems arise when human designers attempt to build AI systems, they are supercharged by the abilities of AIs to \textit{learn, modify themselves and create successors}.
				This implies that even AIs with bug-free implementation and initially good, aligned behavior, can \textit{evolve} to become unaligned, including unintended and damaging behavior.
				\textit{Save} self-improving AI must be free of bugs, aligned and able to design successors (or modify itself) that are also "bug-free" as well as aligned.
				So accidents or screw-ups, even of an fully aligned AI system, may create successor AIs with not any longer human compatible moral values.
				
		\paragraph[Danger of AGI]{The danger of AGI}
			stems primarily from the conflict between the AGIs attainment of its goals, its unfathomable power in doing so, and humanity's pursuit of its own goals.
			
			It is not implied that this is the only source of danger but only that, the threats implied by its nature, establish a lower bound for the danger posed by AGI.
			
			Eliezer \textcite{Yudkowsky22} roughly summarizes the current frontier of understanding in the literature on the danger from AI as follows:
			\begin{enumerate}
				\item AGI will not be upper-bounded by human ability or human learning speed. Things much smarter than human would be able to learn from much less evidence than humans require.
				\item  A cognitive system with sufficiently high cognitive powers, given any medium-bandwidth channel of causal influence, will not find it difficult to bootstrap to overpowering capabilities independent of human infrastructure.
				\item  	A conflict with a high-powered cognitive system looks at least as deadly as "everybody on the face of the Earth suddenly falls over dead within the same second".
				\item 	We need to get alignment right on the first and critical try at operating at a 'dangerous' level of intelligence, operation at a dangerous level of intelligence kills everybody on Earth and then we don't get to try again. 
				\item 	"We" can't just "decide not to build AGI" because it is becoming much easier over time and others will.	The given lethal challenge is, to solve the alignment problem within a time limit.
				\item 	We can't just build a very weak system because those will not be very useful.
				\item 	We need to align the performance of some large task, a 'pivotal act' that prevents other people from building an unaligned AGI that destroys the world.
				\item 	The intense search for ‘pivotal acts' has not produced any candidates, there might just be no such pivotal weak acts.
				\item 	The best and also "easiest-found-by-optimization" algorithms for solving problems we want an AI to solve, readily generalize to problems we would rather the AI not solve (domain specificity is an additional, hard constraint).
				\item 	Running AGIs doing something pivotal can not be passively safe.
			\end{enumerate}
			\textcite{bostrom2014superintelligence} expects that \textit{"[AGIs] —either as a single being or as a new species—become much more powerful than humans, and displace them"}.
			Those and similar concerns have already inspired a public call for AI safety research, by figures such as Stephen Hawking, AAAI president Thomas Dietterich, Eric Horvitz, Bart Selman, Francesca Rossi, Yann LeCun, and the founders of Vicarious and Google DeepMind, in 2015\footcite{RussellLetter}.
					
		\paragraph[Relevance of cognitive abilities]{The Stakes of align- and deployment }
			could not be greater.
			Generally, 'AI takeover' is not expected to be a peaceful transfer of control from humanity to superintelligent agents but rather the end of humanity altogether\footcite{mclean2021risks}.
			Given these concerning reports from the literature, the bleak expectations of experts and the robustness of the case for the severity of the alignment problem, our expectation for AGI ought to be very pessimistic.
			
			A host of examples have been developed to illustrate the significance of superior intelligence\footnote{In accordance with \textcite{bostrom2014superintelligence}, \textit{intelligence} is defined very narrowly as "the capacity for instrumental reasoning". Skill at planning, predicting and mean-ends reasoning, performed to achieve any goal, are the type of instrumental rationality relevant for our considerations. This type of intelligence in searching for instrumentally best policies and plans is compatible with any ultimate or final goal. Nevertheless, neither superintelligence nor AGI require complete instrumental rationality in all domains.}.
			The analogy of our relation with mountain gorillas, physically vastly superior to any human, should suffice to elicit the relevant intuitions.
			Today their species is entirely dependent on our goodwill for their continued existence.
			If humans decided to eradicate them, they would have absolutely no chance stop us.
			Indeed, with many species (think of predators like wolves in Europe) we have demonstrated our willingness to do so, when we perceived them as threat or nuisance.
			This is barring the fact that our similar, biological evolution has arguably inherited us with a significant degree of empathy for them and with values which let us take them into consideration.

			AGI or superintelligence is considered \textit{transformative} in the sense that, conceptually, the existence of even just one such powerful reasoner appears to imply that, what humans value in the world will either increase exponentially or vanish entirely.
			Practically, we should not expect the outcome of AGI to fall in the middle ground between the best or worst case outcomes.
			
			\subparagraph[Failed alignment]{The Apocalypse}
				seems to be the default as solving the alignment problem appears vastly more difficult than creating a superintelligence and an unaligned AGI, which would in all probability eliminate humans altogether.
			
			\subparagraph[Successful alignment]{Paradise}
				appears as the flip-side if humanity indeed succeeded in aligning a superintelligence because the incredible power of such AGI could be leveraged to effectively solve all of humanities current problems.
				It lies outside the scope of this paper to detail how the power of AGI could bring about paradise, readers may refer to the writing on the 'singularity‘\footcite{kruger2021singularity} as first defined by Ray Kurzweil.
				For now we shall assume, that there are appealing arguments for massive opportunity costs of postponing aligned AGI, which are likely to be raised in political debates, and that there are strong economic incentives to pursue such technologies.
				
		\paragraph{The 'unilateralist's curse'}
			refers to the problem\footcite{bostrom2016unilateralist} that the larger the number of agents, the higher the probability, that an action, affecting a set of altruistic agents and whose net value is unknown but probably negative, occurs.
			When each agent acts on her personal judgment, the action will be chosen more often than optimal.
			
			This problem comes to bear even for developers of AGI who are well intended.
			They find themselves in a situation where the more time they spend on improving the alignment and safety of their potential superintelligence, the greater becomes the proliferation of the required technology and knowledge.
			They understand that a consequence of this proliferation is, that the set of other agents who are able to develop AGI increases.
			As the number of agents who are continue to defer increases, the probability of another agent activating a less aligned or secure AGI also increases.
			The incentive for each agent to activate their AGI thus becomes stronger over time or, conversely: their bar of accepting delays in favor of improved safety lowers.
			
			The 'unilateralist's curse' compounds all difficulties in the regulation of AGI and strains the environment of debates about the topic by introducing time pressure.
				
		\subsubsection{Epistemic Problems in the Anticipated Political Debates about AI}
			Try now to imagine the political debate that might arise when the public realizes that transformative AGI is on the horizon.
			We shall imagine the debate, how it is likely to play out without strong previous intervention, and try to consider how it can could improved.
			
			For the sake of simplicity we shall restrict the focus of these considerations to the case of western, liberal democracies.
			While this assumption limits the generality of the conclusions, it nevertheless helps to illustrate some of the key issues.
			Furthermore, it could be argued that, at this time, the emergence of such technology continues to appear most likely in such economies\footnote{Think for example of the share of global tech talent that silicon valley continues to attract.}.
			This is an attempt to anticipate something similar to the debate, that a benevolent first mover at the brink of developing AGI would find herself in.
			
			According to the previous illustration, the prior for any such debate seems to be that AGI will eliminate all humans.
			This is a crucial consideration because our political decision making has no precedence\footnote{Nuclear weapons first enabled humanity to annihilate itself but that risk still comes from their use by humans. This appears to be different in the case of AGI, which has the potential to be destructive "at its own hands".} for such technology and its default decision making procedures appear willfully inadequate.
			
			Generally, this section refrains from justifying individual empirical claims.
			This is because the justification of such broad claims would require the format of literature reviews and no individual claim is crucial to the overall argument.
			Readers will likely agree intuitively but it is not essential to agree with all statements as agreement with some is sufficient for the argument to work.
			
			Another caveat has to be, that the psychological dimensions of trusting AGI is entirely omitted from this discussion.
			The existing literature identifies trust in artificial intelligence as a key issue but with complicated mechanisms and effects in both directions.
			Overall it seems possible in principle, but highly unlikely, that the complex intuitions humans display in trusting AIs could compensate for the weaknesses of collective decision making that will be discussed.
			
			Ultimately, inferences about potential future debates remain speculations but they are worthwhile if they help to be better prepared when the debate arises.
			First, consider the timing and perspective of such a debate.
			
			\paragraph[Timing and burden of prove]{Slowness of decision processes - with liberal defaults for burdens of proof}
				The regulation of complex, difficult to understand and dangerous technologies tends to be historically absent from the public debate.
				Public debates about new technologies tend to begin only when the population starts to feel their impacts.
				In the case of transformative AGI this would clearly be too late.
				Also, usually there is a critical delay between the conclusion of a political debate and the enactment of the respective laws, and even more so for the eventual enforcement of new regulations.
				This is exacerbated by the liberal principles which govern the development of new technologies and the lenient nature of regulations in new sectors.
				
				Today, new technologies are generally legal until it has been proven that they are harmful in a significant way.
				These types of regulation have worked for previous technologies only because societies were able to figure out how to regulate and adopt to them by trial and error.
			
			\paragraph[Reversed restrictions and burden of prove]{Lifting of restrictions - with a restrictive default for burden of proof}
				Given that humanity only has one shot at deploying AGI --- that we have to get it right on the first try as there will only be one try --- ex-post learning is not possible.
				If humanity is faced with nefarious or even just "not" overly cautious developers of AGI, slow legal procedures which only play catch up, will fail to keep up, and result in catastrophe.
				Strict prohibition is therefore required, long before AGI seems imminent, to allow a deliberate and careful debate to even occur.
				The only chance for a epistemically solid debate is, for deliberation to start well in advance and for any research towards AGI to be generally illegal, unless proven safe.
			
			\paragraph{Sovereignty of nations}
				Today's political order takes the sovereignty of nations for granted, unfortunately the fallout of an (rouge) AGI would not.
				While respect for the decisions of other nations and non-interference with their internal affairs and domestic policies is fundamental for peace and order, such principle of non-interference undermine any policies aimed at controlling the development of AGI.
				Deviation of a single agent or nation could doom all others, therefore collective agreements must be reached and enforced on the level of all of humanity.
					
			\paragraph{Dominance of collective interests}
				Even if the political debate and laws governing AGI research err on the side of extreme caution in one country, chances are that there is a host of other countries with less stringent laws.
				
				
				Which actions to take, to influence the policies of foreign nations in this regard, must be subject of the domestic debates as well.
				This principle appears much less controversial, when the existential risk from research and development of AGI is seen as a negative externality on others.
				Preventing agents from engaging in such risky behavior is then just an attempt to stop them from imposing those externalities.
				
				
			\paragraph{Public mistrust of experts}
				In complex and difficult matters, where detailed knowledge of a topic is required for assessment, deferral to experts is usually the epistemically wise choice.
				Unfortunately over the past decades the public's trust in officials, academics and other institutions and experts has rapidly declined.
				In fact, there are numerous examples for continued divergences between scientific consensus and public opinion.
				GMOs, nuclear fission power and climate change are all illustrative examples.
				More often than not, in those cases government and policy side with voters instead of scientists.
				
				In the complicated case of AGI safety and policy, where very emotionally appealing arguments in favor of rapid develop- and deployment can be made\footnote{Think of the ways a super powerful general reasoner could improve the lives of humans.}, and it stands to fear that, even if the expert consensus remained clear, politicians would have incentives to sway with their voters. 
				The Covid pandemic and debates around vaccination have exposed this critical tendency even in the face of high stakes.
					
			\paragraph{Researchers as key authorities}
				When we try to imagine a world that could successfully navigate the proliferation of AGI, it seems that experts there would have at least veto powers to any easing of restrictions as well as emergency powers to rapidly react to dangerous developments.
				In such a surviving world, expert opinion would inform both the public and politics, while more inspiring than dictating opinions.
				Obviously, experts differ in their estimates, opinions and assessment but given the high stakes, the decision making should be inspired by the most conservative and pessimistic estimates.
%				In face of such heterogeneity 
				
				A situation in which experts trust in public decision making because they know their research findings will be meaningfully accounted for, is also one which reduces the epistemic problem of experts being more alarmist than they truly are, because they know that the public decision making will not be responsive to the information they provide.
				
			\paragraph{Lobbying}
				is a pervasive and often non-democratic influence on politics which can devastate the efficiency of information processing and decision making.
				Not even speaking of cases where it constitutes corruption, such activities bias the political system towards the interests of economically strong agents.
				When they, or one of them, can be expected to internalize a majority of the profits of AGI, lobbying my fatally tilt the decision procedures towards leniency.
				If aspects of new, complicated, lethal and difficult to understand technologies become subjects of public debates, those debates often become highly emotional and are hijacked by political interest groups.
				
			\paragraph{No influence of economic interests}
				can be allowed, in either the public debates about AGI or the institutions that are supposed to govern it.
				'Epistemic neutrality' in the sense of freedom of business interests and sober search for truth, is a key requirement for adequately valuing long-term outcomes in the decision procedures.

			\paragraph{Examples}
				A conceivable argument against this pessimistic exposition is that, 'if the stakes are really high, the system does kick into gear and works in preventing the worst outcomes'.
				While the hope, that would be the case, is natural, historic examples\footnote{Because of selection bias, we know only of the cases for which this holds. Dangerous technologies that were developed but successfully kept secret, would be evidence to the contrary but are understandable difficult to obtain.} seem to point towards a different reality.
				
				\subparagraph{Nuclear Weapons}
					loom over humanity and to this day threaten our continued existence.
					Despite these stakes the past three generations of politicians, social activists and citizens have not been able to eliminate the possibility of nuclear holocaust.
				
				\subparagraph[Soviet Bio-weapons program]{The Soviet Bio-weapons program }
					and to some extend \textbf{'Gain of function' research} are two more sobering examples of activities which pose an existential risk to humanity but have been (and are) conducted nevertheless.
					These cases illustrate, that risks to all of humanity are best understood as a negative externality, that neither the agents who create them nor the systems that are meant to oversee them, account for.
			
			\paragraph{What success does and does not look like}
				in the political environment and debate about AGI requires further investigation but it should be evident that decisions along the lines "we do not think it is safe, so we won't do it" are not promising.
				An example for a more promising framing would presumably be: "we can not be entirely sure whether it is safe, so we wont do it and we debate which measures to take to prevent anybody else from doing it".

		\subsubsection{Conclusion}
			The previous discussion has highlighted how our political decision making systematically fails to adequately consider high risk, but at first sight low probability cases.
			The mechanisms, which cause some of the deviation from rational cost-benefit calculation with consideration for all of humanity, presumably lead to catastrophic failure with the emergence of AGI.
			
			Bolstering the previous discussion requires the verification of a number of empirical claims that seem plausible but can be investigated in more detail.
			The same holds true for the technical aspects of AI safety and AGI alignment research.
			
			Nevertheless, because so much is at stake we can not afford the luxury of finishing our theoretical investigations before taking action.
			It appears that, in order to even finish such research, we must urgently change the policy making regimes for this domain towards a 'prohibit first, liberalize later' regime.
			We have to also understand that a debate about what a benevolent first mover should do, might become impossible if the external circumstances change.
			
			In the face of these challenges radical proposals have to be considered.
			One such candidate is to distinguish between commercial AI applications and anything remotely approximating AGI, and to then classify AGI as 'weapons of mass destruction' technology, maximally sanction it and move it from the public realm into the domain of the military and security services.
			
			Some scenarios that may make even such situations extremely dire, are the stresses induced rapid climate, great power competition and arms races.
			Just imagine how much more difficult it would be to postpone research if competition\footnote{May it be China, Russia, Google, Facebook or any other agent.} is perceived to be further ahead in such a 'winner takes it all' scenario.
			
				
	\newpage \part*{Bibliography}
		\pagenumbering{Roman} \setcounter{page}{1}    

    \printbibliography
    \newpage
    
%    \section*{Appendix}

					
    \newpage

		\flushright \includegraphics[width = 40mm]{backgroundsealwatermark.png}\\[2cm]
		        \centering 
\part*{Affidavit}

	    \vspace{2cm}
    \section*{Declaration of Academic Honesty}
	    
	    Hereby, I declare that I have composed the presented paper independently on my own and without any other resources than the ones indicated. All thoughts taken directly or indirectly from external sources are properly denoted as such.
	     \vspace{\baselineskip}
	    \\  This paper has neither been previously submitted to another authority nor has it been published yet.
	    \vspace{6cm}
	    
    \flushright
    \begin{minipage}{0.5\textwidth}
        \begin{flushleft} \large
        \textsc{Munich}                     %   Place
        on the \\ 19th of September 2022                %   Date
        \vspace{2cm}\\
    	{\rule[-3pt]{\linewidth}{.4pt}\par\smallskip  
        \textsc{Valentin Meyer}	\\         %   Your name
    	}
        \end{flushleft}
        \end{minipage}

    \begin{comment}
        Tips for sources:
        Good Journals:
        	Top5 (AmericanEconomicReview, Journal of Political Economy, Econometrica, Quarterly Journal of Economics, Review of Economics Studies);
            NBER, CESifo etc Working Paper;
            Good Field Journals;
            Handelsblatt Ranking etc.
        	Always ask:
            	Do they conduct proper Peer-review?
                Who are the authors?
           Careful with Thinktanks, research whether they are biased
        Cite your supervisour
            Always a nice move
            Make extra sure you understand their paper
            Unfortunate spot for a mistake
        The References (Bibliography) should contain only that sources that you are referringto in the text, listed in alphabetical orler:oMonographs: surname, given name(s) resp. initials, (editor), year, title,subtitle, edition, volume, place of publication.
    Journal articles: surname, given name(s), year, title of the article, title of thejournal, volume, page numbers.
    Articles in miscellanies: surname, given name(s), year, title of the article, in:surname, name(s), (editor), year, title, ..., page numbers.
    If in doubt, have a look at the bibliography in papers that are published in the American Economic Review.
    •Sources of all figures and tables which you have adopted from other texts have to be cited precisely (i.e. including the exact page)
    
\end{comment}
  
%    \tableofcontents
\end{document}
\newpage \part{Full Appendix/Appendices}
				


		\paragraph{The role of trust for the political debate}
		
			
			\subparagraph{Trust without understanding}		
				Children trust their parents etc. 
				(Even Farmed) Animals trust their owners
				
			Role of benevolence 
				
) Predictability through design competence

2) Predictability through inheritance

The system's implementation may contain initially-unnoticed routine but catastrophic bugs.
An analogy is space probes: despite the knowledge that bugs in expensive space probes are hard to fix after launch, engineers have historically not been able to prevent catastrophic bugs from occurring.[10][28]

No matter how much time is put into pre-deployment design, a system's specifications often result in unintended behavior the first time it encounters a new scenario.
For example, Microsoft's Tay behaved inoffensively during pre-deployment testing, but was too easily baited into offensive behavior when interacting with real users.[9]

3) Predictability through convergent instrumental reasons
	
	As seen before, highly concerning


			\subparagraph{Trust as evolutionary relic}
			
			That we are able to trust without understanding is an evolutionary and cultural relict. 

			
			\subparagraph{Our trusting intuition as weakness in strategic interactions}
			
					In strategic interactions it is a flaw.
					Evolution (and culture) ill prepared us - trust as weakness?
					
					
			\subparagraph{Trust as (in-) sufficient condition for safe deployment}
				NO!
			
			
			
			
		\paragraph{The paradox of understanding}
			Can we ever understand a vastly more intelligent agent?
				Intuitively no. 
				Seems as sometimes we might
					If the internal processes are clear and "intelligence" stems from the 'ability to process vast quantities of data'
					
				Never for a superior general reasoner?
				Epistemic problem
				Can know what we dont know
				 
				 Maybe collective intelligence as somewhat of a remedy?



    %   numbered and headed, e.g. A1, A2 ...
    %   All content (Tables, figures etc.) must be referred to in the text
    \section{More extensive calculations/derivations}
    \section{Additional Materials}
        \subsection{Additional Tables}  
        \subsection{Additional Figures}
    \section{Index}
    \section{List of Abbreviations}
    \section{Symbol directory/ List of symbols}
    \section{Table of Figures / Register of Illustrations}
    \section{Index of Tables}
    \section{Table of Equations/Formulas}
    \section{Glossary}
    \printglossaries                    %   This command prints the glossary
    
    \section{Detailed structure of this paper}
    To provide a detailed and precise overview of this paper I include another table of contents that can be used similar to a glossary. \tableofcontents
